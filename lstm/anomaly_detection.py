# -*- coding: utf-8 -*-
"""anomaly_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k5avM1PTX74XLhGGmNjDQbi7xv0OfTqZ
"""

import pandas as pd
import numpy as np
import collections
import matplotlib
import seaborn
import matplotlib.dates as md
from matplotlib import pyplot as plt
from sklearn import preprocessing
import torch
import cv2

# @title Plot 10-value window
def unroll(data,sequence_length):
    result = []
    x = data.reset_index()['x']
    # x = data['x']
    y = data.reset_index()['y']
    z = data.reset_index()['z']
    r = data.reset_index()['r']
    # y = data['y']
    # z = data['z']
    # r = data['r']
    x_res = []
    x_next = []
    y_res = []
    z_res = []
    r_res = []

    for index in range(len(data) - sequence_length):
        # d = data[index: index + sequence_length]
        # flat_list = [item for sublist in d for item in sublist]
        result.append(data[index: index + sequence_length])
        x_res.append(x[index: index + sequence_length])
        x_next.append(x[index + sequence_length])
        y_res.append(y[index: index + sequence_length])
        z_res.append(z[index: index + sequence_length])
        r_res.append(r[index: index + sequence_length])
        # result.append(flat_list)
    return np.asarray(result), x_res, x_next, y_res, z_res, r_res
for i in range(11,21):
  # fig,ax = plt.subplots()
  test = plt.figure()
  # file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  file_name = 'test/danny/danny flight ' + str(i) + '.csv'
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                    & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                    &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  data_n = pd.DataFrame(data_n)
  data_seq, x_res, x_next, y_res, z_res, r_res = unroll(data_n, 10)
  for j in range(len(x_res)):
    num_unique = np.unique(x_res[j])
    if len(num_unique) > 1:
      # x = x_res[j].reset_index()['x']
      # v = []
      # for z in range(1,len(x)):
      #   v.append(x[z] - x[z-1])
      v =x_res[j].reset_index()['x']
      plt.plot(v - v[0],color='r', alpha = 0.1)
  plt.show()
  # cv2.imwrite('./classification/danny/danny plot ' + str(i) + '.png' , ax)
  # test.savefig('./classification/fengchen/fengchen plot ' + str(i) + '.png')
  # from google.colab import files
  # test.savefig("abc.png")
  test.savefig('./test/danny/danny plot ' + str(i) + '.png')

for i in range(20,30):
  # fig,ax = plt.subplots()
  test = plt.figure()
  file_name = 'test/fengchen/fengchen_flight' + str(i) + '.csv'
  # file_name = 'test/danny/danny flight ' + str(i) + '.csv'
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                    & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                    &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  data_n = pd.DataFrame(data_n)
  data_seq, x_res, x_next, y_res, z_res, r_res = unroll(data_n, 10)
  for j in range(len(x_res)):
    num_unique = np.unique(x_res[j])
    if len(num_unique) > 1:
      # x = x_res[j].reset_index()['x']
      # v = []
      # for z in range(1,len(x)):
      #   v.append(x[z] - x[z-1])
      v =x_res[j].reset_index()['x']
      plt.plot(v - v[0],color='r', alpha = 0.1)
  plt.show()
  # cv2.imwrite('./classification/danny/danny plot ' + str(i) + '.png' , ax)
  test.savefig('./test/fengchen/fengchen plot ' + str(i) + '.png')
  # from google.colab import files
  # test.savefig("abc.png")
  # test.savefig('./test/danny/danny plot ' + str(i) + '.png')

#@title
fig,ax = plt.subplots()
c = ['r', 'g', 'b', 'cyan']
for i in range(1,2):
  file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  # file_name = 'danny/danny flight ' + str(i) + '.csv'
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                    & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                    &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  data_seq, x_res, y_res, z_res, r_res = unroll(data_n, 10)
  for j in range(len(x_res)):
    num_unique = np.unique(x_res[j])
    if len(num_unique) > 1:
      # x = x_res[j].reset_index()['x']
      # v = []
      # for z in range(1,len(x)):
      #   v.append(x[z] - x[z-1])
      v =x_res[j].reset_index()['x']
      ax.plot(v - v[0],color=c[i-1], alpha = 0.1)
plt.show()

import random
i = random.choice([0,1])
if i == 0:
    file_name = 'fengchen/fengchen flight' + str(8) + '.csv'
else:
    file_name = 'danny/danny flight ' + str(8) + '.csv'
df = pd.read_csv(file_name)
data_n = df[['x', 'y', 'z', 'r']]
data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                  & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                  &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
data_seq, x_res, y_res, z_res, r_res = unroll(data_n, 10)
fig,ax = plt.subplots()
# for j in range(len(x_res)):
j = random.randint(1, 1000)

while True:
  num_unique = np.unique(x_res[j])
  if len(num_unique) > 1:
    # v =x_res[j].reset_index()['x']
    # x = x_res[j].reset_index()['x']
    # v = []
    # for z in range(1,len(x)):
      # v.append(x[z] - x[z-1])
    v =x_res[j].reset_index()['x']
    # ax.plot(v,color='r', alpha = 1)
    ax.plot(v-v[0] ,color='r', alpha = 1)
    break
  else:
    j = random.randint(1, 1000)
plt.show()

"""Manual classification

Classification of D: 71.84%

Classification of F: 87.97%
"""

import math
def manual_classification(data):
  rate = []
  data = data.reset_index()['x']
  for z in range(1,len(data)):
    rate.append(data[z] - data[z-1])
  rate_0_count = rate.count(0)
  if rate_0_count == 0:
    return 'd'
  prev = None
  for i in rate:
      if prev != None and i != prev and i != 0 and prev != 0:
        return 'd'
      prev = i
  if rate_0_count < math.floor(len(rate) / 2):
    return 'd'
  return 'f'
    # if rate_0_count == 0 or rate_0_count < math.floor(len(rate) / 2): # No zero rate or # of zero rate less than half
  #   return 'd'
  # return 'f'

total = []
for i in range(1,11):
  file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  # file_name = 'danny/danny flight ' + str(i) + '.csv'
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                    & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                    &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  data_seq, x_res, y_res, z_res, r_res = unroll(data_n, 10)
  pred = []
  for j in x_res:
    num_unique = np.unique(j)
    if len(num_unique) > 1:
      pred.append(manual_classification(j))
      # if manual_classification(j) == 'd':
      #   print(j)
  error = [i for i, x in enumerate(pred) if x == "d"]
  f = pred.count('f')
  d = pred.count('d')
  total.append(f / (d+f))
#   print('d: ', d)
#   print('f: ', f)
#   print('f/d: ', f / (d+f))
# print('avg: ', sum(total) / len(total))

# predicted danny error
error

# predicted fengchen error
error

"""SVM / Decision Tree
Only decision tree is accurate
"""

!pip3 install pyod

#@title Train
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.ensemble import IsolationForest
# from pyod.models.ecod import ECOD
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.neighbors import LocalOutlierFactor
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
import math
import statistics
# clf = svm.SVC() #kernel = 'poly'
# clf = DecisionTreeClassifier()
# clf = IsolationForest(random_state=0)
# clf = svm.OneClassSVM(nu = 0.2)
# clf = ECOD()
# clf = LogisticRegression(random_state=0) # needs at least 2 classes in y
clf = KMeans(n_clusters=8, random_state=0)
# clf = LocalOutlierFactor(novelty=True)
# clf = RandomForestClassifier(random_state=0)
score1 = []
score2 = []
X = []
y = []
for i in range(1,6):
  # file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  file_name = 'danny/danny flight ' + str(i) + '.csv'
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                    & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                    &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  data_seq1, x_res1, x_next1, y_res1, z_res1, r_res1 = unroll(data_n, 10)
  
  v1 = []
  target1 = []
  for index, j in enumerate(x_res1):
    num_unique = np.unique(j)
    if len(num_unique) > 1:
      x = j.reset_index()['x']
      res = []
      rate = []
      for z in range(1,len(x)):
        # res.append(x[z-1])
        rate.append(x[z] - x[z-1])
        # res.append(x[z] - x[z-1])
      rate_0_count = rate.count(0)
      rate_num_unique = len(set(rate))
      prev = None
      cnt = 0
      for k in rate:
        if prev != None and k != prev and k != 0 and prev != 0:
          cnt += 1
        prev = i
        # len(num_unique), 
      # res += [len(num_unique), rate_num_unique, rate_0_count< math.floor(len(rate) / 2), cnt, statistics.stdev(x)]
      res += [statistics.stdev(x)]
      # v1.append([x[0], x[9]])
      # v1.append(x)
      v1.append(res)
      # target1.append(x_next1[index])
  # target1 = [0 for z in range(len(v1))]
  target1 = [1 for z in range(len(v1))] # Isolation tree
  # target1 = x_next1 # Regression
  X += v1
  y += target1
  
# for i in range(1,2):
  # file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  '''
  file_name = 'danny/danny flight ' + str(i) + '.csv'
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                    & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                    &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  data_seq2, x_res2, x_next2, y_res2, z_res2, r_res2 = unroll(data_n, 10)
  v2 = []
  target2 = []
  for index, j in enumerate(x_res2):
    num_unique = np.unique(j)
    if len(num_unique) > 1:
      x = j.reset_index()['x']
  '''
      # rate = []
      # for z in range(1,len(x)):
      #   rate.append(x[z] - x[z-1])
      # rate_0_count = rate.count(0)
      # rate_num_unique = len(set(rate))
      # prev = None
      # cnt = 0
      # for k in rate:
      #   if prev != None and k != prev and k != 0 and prev != 0:
      #     cnt += 1
      #   prev = i
      # v2.append([len(num_unique), rate_num_unique, rate_0_count< math.floor(len(rate) / 2), cnt])
      # v2.append([x[0], x[9]])
  '''
      v2.append(x)
  '''
      # target2.append(x_next2[index])
  # target2 = [1 for z in range(len(v2))]
  # target2 = x_next2
  # X += v2
  # y += target2
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test
# clf = clf.fit(X_train,y_train)
clf = clf.fit(X_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
y_train = clf.predict(X_train)
# percentile = np.percentile(clf.decision_function(X),20)
# X_test = np.asarray(X_test)
# dist = []
# for index, v in enumerate(y_pred):
#   dist.append(np.linalg.norm(X_test[index]-clf.cluster_centers_[v]))
# print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
# print(percentile)
# avg = sum(dist) / len(dist)
# print("distance:", avg)
# clf.fit(X, y)
# score1.append(clf.score(X, y))
  # score2.append(clf.score(v2, target2))
# fengchen is 0, danny is 1
# inliner is 1, outlier is -1

# clf.score(X_train)
# print(a[0])
clf.cluster_centers_
avg_train = 0
avg_test = 0
for index, v in enumerate(y_train):
  avg_train += np.linalg.norm(X_train[index]-clf.cluster_centers_[v]) / len(y_train)
for index, v in enumerate(y_pred):
  avg_test += np.linalg.norm(X_test[index]-clf.cluster_centers_[v]) / len(y_pred)
print(avg_train)
print(avg_test)

from sklearn import tree
tree.plot_tree(clf)

fig,ax = plt.subplots()
ax.plot(score2 ,color='r', alpha = 1)
plt.show()

#@title Test
# ##########TEST##############
acc = []
for i in range(6, 7):
  file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                    & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                    &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  data_seq1, x_res1, x_next1, y_res1, z_res1, r_res1 = unroll(data_n, 10)
  v1 = []
  target1 = []
  for index, j in enumerate(x_res1):
    num_unique = np.unique(j)
    if len(num_unique) > 1:
      x = j.reset_index()['x']
      res = []
      rate = []
      for z in range(1,len(x)):
        # res.append(x[z-1])
        rate.append(x[z] - x[z-1])
        # res.append(x[z] - x[z-1])
      rate_0_count = rate.count(0)
      rate_num_unique = len(set(rate))
      prev = None
      cnt = 0
      for k in rate:
        if prev != None and k != prev and k != 0 and prev != 0:
          cnt += 1
        prev = i
      # res += [len(num_unique), rate_num_unique, rate_0_count< math.floor(len(rate) / 2), cnt, statistics.stdev(x)]
      res += [statistics.stdev(x)]
      # v1.append([x[0], x[9]])
      # v1.append(x)
      v1.append(res)
      # target1.append(x_next1[index])
  # target1 = [0 for k in range(len(v1))]
  target1 = [1 for k in range(len(v1))] # Isolation tree
  file_name = 'danny/danny flight ' + str(i) + '.csv'
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                    & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                    &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  data_seq2, x_res2, x_next2, y_res2, z_res2, r_res2 = unroll(data_n, 10)
  v2 = []
  target2 = []
  for index, j in enumerate(x_res2):
    num_unique = np.unique(j)
    if len(num_unique) > 1:
      x = j.reset_index()['x']
      res = []
      rate = []
      for z in range(1,len(x)):
        # res.append(x[z-1])
        rate.append(x[z] - x[z-1])
        # res.append(x[z] - x[z-1])
      rate_0_count = rate.count(0)
      rate_num_unique = len(set(rate))
      prev = None
      cnt = 0
      for k in rate:
        if prev != None and k != prev and k != 0 and prev != 0:
          cnt += 1
        prev = i
        # len(num_unique),
      # res += [len(num_unique), rate_num_unique, rate_0_count< math.floor(len(rate) / 2), cnt, statistics.stdev(x)]
      res += [statistics.stdev(x)]
      # v2.append([x[0], x[9]])
      # v2.append(x)
      v2.append(res)
      # target2.append(x_next2[index])
  # target2 = [1 for k in range(len(v2))]
  target2 = [-1 for k in range(len(v2))] # Isolation tree
  # y_pred1 = clf.decision_function(v1)
  # y_pred2 = clf.decision_function(v2)
  pred_F = clf.predict(v1)
  pred_D = clf.predict(v2)
  # v1 = np.asarray(v1)
  # v2 = np.asarray(v2)
  t1 = []
  t2 = []
  for index, v in enumerate(pred_F):
    if v == 0 or v == 1:
      continue
    if np.linalg.norm(v1[index]-clf.cluster_centers_[v]) < avg_train:
      t1.append(0)
    else:
      t1.append(1)
  for index, v in enumerate(pred_D):
    if v == 0 or v == 1:
      continue
    if np.linalg.norm(v2[index]-clf.cluster_centers_[v]) < avg_train:
      t2.append(0)
    else:
      t2.append(1)
  # print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
  # for k in y_pred1:
  #   if k < percentile:   #The lower, the more abnormal
  #     t1.append(1)
  #   else:
  #     t1.append(0)
  # for k in y_pred2:
  #   if k < percentile:   #The lower, the more abnormal
  #     t2.append(1)
  #   else:
  #     t2.append(0)
  # avg_D = 0
  # avg_F = 0

  # for index, v in enumerate(pred_F):
  #   avg_F += np.linalg.norm(v1[index]-clf.cluster_centers_[v]) / len(pred_F)
  # for index, v in enumerate(pred_D):
  #   avg_D += np.linalg.norm(v2[index]-clf.cluster_centers_[v]) / len(pred_D)
  print('avg_F: ', t1.count(1) / len(t1))
  print('avg_D: ', t2.count(0) / len(t2))

  plt.hlines(1,1,20)  # Draw a horizontal line
  plt.xlim(0,max(v1)[0])
  plt.ylim(0.5,1.5)

  y = np.ones(np.shape(v1))   # Make all y values the same
  plt.plot(v1,y,'|',ms = 40)  # Plot a line at each location specified in a
  y2 = np.ones(np.shape(clf.cluster_centers_))
  plt.plot(clf.cluster_centers_, y2, '|', color = 'r', ms=60)
  plt.axis('off')
  plt.show()

  plt.hlines(1,1,20)  # Draw a horizontal line
  plt.xlim(0,max(v2)[0])
  plt.ylim(0.5,1.5)

  y = np.ones(np.shape(v2))   # Make all y values the same
  plt.plot(v2,y,'|',ms = 40)  # Plot a line at each location specified in a
  y2 = np.ones(np.shape(clf.cluster_centers_))
  plt.plot(clf.cluster_centers_, y2, '|', color = 'r', ms=60)
  plt.axis('off')
  plt.show()
  # print('D prediction: ', metrics.mean_squared_error(clf.predict(np.asarray(v2)), target2))
  # print('F prediction: ', metrics.mean_squared_error(clf.predict(np.asarray(v1)), target1))
#   print('D prediction: ', metrics.accuracy_score(clf.predict(np.asarray(v2)), target2))
#   acc.append(metrics.accuracy_score(clf.predict(np.asarray(v2)), target2))
#   print('F prediction: ', metrics.accuracy_score(clf.predict(np.asarray(v1)), target1))
# print('Avg D accuracy: ', sum(acc) / len(acc))

fig,ax = plt.subplots()
ax.plot(clf.decision_function(np.asarray(v1)))
ax.plot(0, color = 'r')
plt.show()
# np.sort(clf.decision_function(np.asarray(v1)))

fig,ax = plt.subplots()
ax.plot(clf.decision_function(np.asarray(v2)))
plt.show()

#@title Default title text
fig,ax = plt.subplots()
# j = 218
# ax.plot(v1[j] ,color='r', alpha = 1)
# plt.show()
x = 109
cnt = 0
i = 10
# file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
file_name = 'danny/danny flight ' + str(i) + '.csv'
df = pd.read_csv(file_name)
data_n = df[['x', 'y', 'z', 'r']]
data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                  & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                  &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
data_seq, x_res, y_res, z_res, r_res = unroll(data_n, 10)
for j in x_res:
    num_unique = np.unique(j)
    if len(num_unique) > 1:
      if cnt == x:
        v = j.reset_index()['x']
        ax.plot(v-v[0] ,color='r', alpha = 1)
        plt.show()
        break
      cnt += 1





"""Classification from plots

75% accuracy

Danny 8 is predicted wrong
"""



#@title Image preprocess
import matplotlib.pyplot as plt
import numpy as np
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import pathlib
data_dir = pathlib.Path('./test')
# PIL.Image.open('./classification/danny/danny plot 10.png')
batch_size = 1
img_height = 180
img_width = 230
image_count = len(list(data_dir.glob('*/*.png')))
print(image_count)
train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.3,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
val_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.3,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
class_names = train_ds.class_names
print(class_names)
import matplotlib.pyplot as plt
for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

#@title Model defination
normalization_layer = layers.Rescaling(1./255)
num_classes = len(class_names)

model = Sequential([
  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.summary()

#@title train
epochs=10
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

#@title test
# for i in range(20, 30):
img = tf.keras.utils.load_img(
    # './test/fengchen/fengchen plot ' + str(i) + '.png', 
    # 'danny plot 19.png',
    'fengchen plot 29.png',
    target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(class_names[np.argmax(score)], 100 * np.max(score))
)

"""LSTM Classification"""

file_name = 'danny/danny flight 1.csv'
df = pd.read_csv(file_name)
data_n = df[['x', 'y', 'z', 'r']]
data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
x = data_n.reset_index()
(x.iloc[0][['x', 'y', 'z', 'r']]- [1,2,3,4]) / [2,2,2,2]

#@title Updated x axis (4 axises in comments)
# from __future__ import all_feature_names
from torch.optim import optimizer
from sklearn.model_selection import train_test_split 
from torch.utils.data import Dataset
from torch.autograd import Variable
import os
import random
import statistics
def set_seed(seed):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.backends.cudnn.determinisdatic = True
    torch.backends.cudnn.benchmark = False
    print ("Seeded everything")

def unroll(data,sequence_length, all_mean, all_stdev, out, portion, train):
    x = data.reset_index()['x']
    x_res = []
    y = [0, 0]
    y[out] = 1
    y = torch.tensor(y).float()
    label = []
    count = 0
    half = len(x) / 2
    for index in range(len(data) - sequence_length):
        num_unique = np.unique(x[index: index + sequence_length][0:10])
        if len(num_unique) > 1:
          count += 1
          j = []
          rate = []
          for i in range(index, index + sequence_length):
            rate.append(x[i+1] - x[i])
            if i == 0:
              # j.append([(x[i]- all_mean[0]) / all_stdev[0], 0])
              j.append([0])
            else:
              # j.append([(x[i]- all_mean[0]) / all_stdev[0], x[i] - x[i-1]])
              j.append([x[i] - x[i-1]])
            # j.append((x[i]- all_mean[0]) / all_stdev[0])
          # j.append(statistics.stdev(x[index: index + sequence_length]))
          rate_0_count = rate.count(0)
          rate_num_unique = len(set(rate))
          # j.append(rate_0_count)
          # j.append(rate_num_unique)
          # print(j)
          j = torch.tensor(j).float()
          x_res.append(j)
          label.append(y)
    # if train:
    #   if portion == 'first':
    #     x_res = x_res[:count//2]
    #   else:
    #     x_res = x_res[count//2:]
    return x_res, label

def normalize_data(df_train):
  all_mean = []
  all_stdev = []

  for c in df_train.columns:
      mean = df_train[c].mean()
      stdev = df_train[c].std()
      all_mean.append(mean)
      all_stdev.append(stdev)

  return all_mean, all_stdev

def data_preprocess(file_name, out, portion, train):
  min_max_scaler = preprocessing.StandardScaler()
  print(file_name)
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                  & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                  &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  all_mean, all_stdev = normalize_data(data_n)
  X, Y = unroll(data_n, 10, all_mean, all_stdev, out, portion, train)

  return X, Y

class SequenceDataset(Dataset):
    def __init__(self, x, y):
        self.y = y
        self.X = x

    def __len__(self):
        return len(self.X)

    def __getitem__(self, i): 

        return self.X[i], self.y[i]

set_seed(1)
batch_size = 16
X = []
Y = []
for i in range(5,15):
  file_name = 'danny/danny flight ' + str(i) + '.csv'
  out = 1
  x, y = data_preprocess(file_name, out, 'first', True)
  X += x
  Y += y
for i in range(1,5) + range(20,24):
  file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  out = 0
  x, y = data_preprocess(file_name, out, 'second', True)
  X += x
  Y += y
dataset = SequenceDataset(X, Y)
torch.manual_seed(99)

# loader = torch.utils.data.DataLoader(dataset = dataset,
#                                     batch_size = batch_size,
#                                     shuffle = True)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])
train_loader = torch.utils.data.DataLoader(dataset = dataset,
                                    batch_size = batch_size,
                                    shuffle = True)
# val_loader = torch.utils.data.DataLoader(dataset = val_dataset,
#                                     batch_size = batch_size,
#                                     shuffle = True)
# train, validate = torch.utils.data.random_split(loader, [0.8, 0.2])

for x,y in train_loader:
  print(x.shape)
  print(y.shape)
  break

#@title Train
from torch import nn
class Classification(torch.nn.Module):
  def __init__(self, num_sensors, hidden_units):
    super(Classification, self).__init__()

    self.num_sensors = num_sensors  # this is the number of features
    self.hidden_units = hidden_units
    self.num_layers = 2

    self.lstm = nn.LSTM(
        input_size=num_sensors,
        hidden_size=hidden_units,
        batch_first=True,
        num_layers=self.num_layers
    )
    torch.nn.init.xavier_normal_(self.lstm.weight_ih_l0)
    torch.nn.init.xavier_normal_(self.lstm.weight_ih_l1)
    self.linear1 = nn.Linear(in_features=self.hidden_units, out_features=2)
    self.relu = nn.ReLU()
    # self.linear2 = nn.Linear(in_features=5, out_features=2)

    torch.nn.init.xavier_normal_(self.linear1.weight)
    # torch.nn.init.xavier_normal_(self.linear2.weight)
    self.sigmoid = nn.Sigmoid()

  def forward(self, x):
    batch_size = x.shape[0]
    h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()
    c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()

    out, (hn, _) = self.lstm(x, (h0, c0))
    out = self.sigmoid(self.linear1(hn[1]))
    # out = self.relu(self.linear1(hn[0]))
    # out = self.sigmoid(self.linear2(out))

    return out
  # output: [Fengchen, Danny]
  # Is danny = 1
  # Not danny = 0

def test_model(data_loader, model, loss_function):
    num_batches = len(data_loader)
    total_loss = 0
    num_dismatch = 0
    outputs = []
    model.eval()
    with torch.no_grad():
        for X,y in data_loader:
            output = model(X)
            outputs.append(output)
            l = loss_function(output, y)
            total_loss += l.item()

    avg_loss = total_loss / num_batches
    return avg_loss, outputs

def train_model(loss_function):
  num_hidden_units = 8
  model = Classification(1, num_hidden_units)
  optimizer = torch.optim.Adam(model.parameters(),
                                  lr = 1e-2,
                                  weight_decay = 1e-6)
  epochs = 50
  train_loss = []
  val_loss = []
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  print("The model will be running on", device, "device")
  # Convert model parameters and buffers to CPU or Cuda
  model.to(device)
  model.train()
  num_batches = len(train_loader)
  for epoch in range(epochs):
    # model.train()
    losses = 0
    # for x, y in loader:
    for x, y in train_loader:
      x = Variable(x.to(device))
      y = Variable(y.to(device))

      optimizer.zero_grad()
      output = model(x)
      loss = loss_function(output, y)
      loss.backward()
      optimizer.step()
      losses += loss.item()
    
    avg_loss = losses / num_batches
    train_loss.append(avg_loss)
    # l, _ = test_model(val_loader, model, loss_function)
    # val_loss.append(l)
    print("Epoch: ", epoch, "Loss: ", avg_loss)

  return model, train_loss, val_loss

loss_function = torch.nn.BCELoss() 
# loss_function = torch.nn.CrossEntropyLoss()
#Train
model, train_loss, val_loss = train_model(loss_function)
# losses_val, outputs = test_model(val_loader, model, loss_function)

print(model)

epochs_range = range(50)

plt.figure(figsize=(8, 8))

plt.plot()
plt.plot(epochs_range, train_loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

#@title Test
batch_size = 1

for i in list(range(1,5)) + list(range(15, 21)):
  X = []
  Y = []  
  file_name = 'danny/danny flight ' + str(i) + '.csv'
  out = 1
  x, y = data_preprocess(file_name, out, 'first', False)
  X += x
  Y += y
  dataset = SequenceDataset(X, Y)
  torch.manual_seed(99)
  data_loader = torch.utils.data.DataLoader(dataset = dataset,
                                      batch_size = batch_size,
                                      shuffle = False)
  num_batches = len(data_loader)
  total_loss = 0
  num_dismatch = 0
  avg_loss, outputs = test_model(data_loader, model, loss_function)
  a = [i.numpy() for i in outputs]
  b = []
  prob_d = []
  prob_f = []
  for c in a:
    prob_f.append(c[0][0])
    prob_d.append(c[0][1])
    if c[0][0] < c[0][1]:
      b.append('D')
    else:
      b.append('F')
  
  print('D: ', b.count('D'))
  print('F: ', b.count('F'))
  # print('D accuracy: ', b.count('D') / (b.count('D') + b.count('F')))
  print('D avg probability: ', sum(prob_d) / len(prob_d))
  # print('F avg probability: ', sum(prob_f) / len(prob_f))
for i in list(range(5,11)) + list(range(24, 30)):
  X = []
  Y = []
  file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  out = 0
  x, y = data_preprocess(file_name, out, 'first', False)
  X += x
  Y += y
  dataset = SequenceDataset(X, Y)
  torch.manual_seed(99)
  data_loader = torch.utils.data.DataLoader(dataset = dataset,
                                      batch_size = batch_size,
                                      shuffle = False)
  num_batches = len(data_loader)
  total_loss = 0
  num_dismatch = 0
  avg_loss, outputs = test_model(data_loader, model, loss_function)
  # print('avg loss: ', avg_loss)
  a = [i.numpy() for i in outputs]
  b = []
  prob_d = []
  prob_f = []
  for c in a:
    prob_f.append(c[0][0])
    prob_d.append(c[0][1])
    if c[0][0] < c[0][1]:
      b.append('D')
    else:
      b.append('F')
  print('D: ', b.count('D'))
  print('F: ', b.count('F'))
  # print('F accuracy: ', b.count('F') / (b.count('D') + b.count('F')))
  # print('D avg probability: ', sum(prob_d) / len(prob_d))
  print('F avg probability: ', sum(prob_f) / len(prob_f))
  # output: [Fengchen, Danny]

"""Autoencoder"""

#@title Preprocess
from torch.optim import optimizer
from sklearn.model_selection import train_test_split 
from torch.utils.data import Dataset
from torch.autograd import Variable
import os
import random
import statistics
def set_seed(seed):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.backends.cudnn.determinisdatic = True
    torch.backends.cudnn.benchmark = False
    print ("Seeded everything")

def unroll(data,sequence_length, all_mean, all_stdev, out):
    x = data.reset_index()['x']
    x_res = []
    y = [0, 0]
    y[out] = 1
    y = torch.tensor(y).float()
    label = []
    diff = []
    diff.append(0)
    for i in range(1, len(x)):
      diff.append(x[i] - x[i-1])
    diff_mean = statistics.mean(diff)
    diff_stdev = statistics.stdev(diff)
    for index in range(len(x) - sequence_length):
        num_unique = np.unique(x[index: index + sequence_length][0:10])
        if len(num_unique) > 1:
          j = []
          rate = []
          for i in range(index, index + sequence_length):
            rate.append(x[i+1] - x[i])
            if i == 0:
              j.append([(x[i]- all_mean[0]) / all_stdev[0], 0])
              # j.append([0])
            else:
              j.append([(x[i]- all_mean[0]) / all_stdev[0], ((x[i] - x[i-1]) - diff_mean) / diff_stdev])
              # j.append([((x[i] - x[i-1]) - diff_mean) / diff_stdev])
            # j.append([(x[i]- all_mean[0]) / all_stdev[0]])
          rate_0_count = rate.count(0)
          rate_num_unique = len(set(rate))
          j = torch.tensor(j).float()
          x_res.append(j)
          label.append(y)
    return x_res, label

def normalize_data(df_train):
  all_mean = []
  all_stdev = []

  for c in df_train.columns:
      mean = df_train[c].mean()
      stdev = df_train[c].std()
      all_mean.append(mean)
      all_stdev.append(stdev)

  return all_mean, all_stdev

def data_preprocess(file_name, out):
  min_max_scaler = preprocessing.StandardScaler()
  print(file_name)
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                  & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                  &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  all_mean, all_stdev = normalize_data(data_n)
  X, Y = unroll(data_n, 10, all_mean, all_stdev, out)

  return X, Y

class SequenceDataset(Dataset):
    def __init__(self, x):
        self.X = x

    def __len__(self):
        return len(self.X)

    def __getitem__(self, i): 

        return self.X[i]

set_seed(1)
batch_size = 16
X = []
Y = []
for i in range(1,5):
  '''
  file_name = 'danny/danny flight ' + str(i) + '.csv'
  out = 1
  x, y = data_preprocess(file_name, out)
  X += x
  Y += y
  '''
  file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  out = 0
  x, y = data_preprocess(file_name, out)
  X += x
  # Y += y
# train_size = int(0.8 * len(dataset))
# test_size = len(dataset) - train_size
# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])
dataset = SequenceDataset(X)
torch.manual_seed(99)
train_loader = torch.utils.data.DataLoader(dataset = dataset,
                                    batch_size = batch_size,
                                    shuffle = True)

i=1
file_name = 'danny/danny flight ' + str(i) + '.csv'
df = pd.read_csv(file_name)
data_n = df[['x', 'y', 'z', 'r']]
data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
data_n = data_n.reset_index()
data_n[277:278]

#@title Use 4 axises in autoencoder
from torch.optim import optimizer
from sklearn.model_selection import train_test_split 
from torch.utils.data import Dataset
from torch.autograd import Variable
import os
import random
import statistics
def set_seed(seed):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.backends.cudnn.determinisdatic = True
    torch.backends.cudnn.benchmark = False
    print ("Seeded everything")
def unroll_4(data,sequence_length, all_mean, all_stdev, out):
    d = data.reset_index()
    x_res = []
    y = [0, 0]
    y[out] = 1
    y = torch.tensor(y).float()
    label = []
    diffx = []
    diffy = []
    diffz = []
    diffr = []
    for i in range(len(d)-1):
        diffx.append(d.iloc[i+1]['x'] - d.iloc[i]['x'])
        diffy.append(d.iloc[i+1]['y'] - d.iloc[i]['y'])
        diffz.append(d.iloc[i+1]['z'] - d.iloc[i]['z'])
        diffr.append(d.iloc[i+1]['r'] - d.iloc[i]['r'])
    mean = [statistics.mean(diffx), statistics.mean(diffy), statistics.mean(diffz), statistics.mean(diffr)]
    std = [statistics.stdev(diffx), statistics.stdev(diffy), statistics.stdev(diffz), statistics.stdev(diffr)]
    for index in range(len(d) - sequence_length):
        x = d[index: index + sequence_length][0:10]['x']
        y = d[index: index + sequence_length][0:10]['y']
        z = d[index: index + sequence_length][0:10]['z']
        r = d[index: index + sequence_length][0:10]['r']
        num_unique_x = np.unique(x)
        num_unique_y = np.unique(y)
        num_unique_z = np.unique(z)
        num_unique_r = np.unique(r)
        if len(num_unique_x) > 1 or len(num_unique_y) > 1 or len(num_unique_y) > 1 or len(num_unique_y) > 1:
          j = []
          for i in range(index, index + sequence_length):
            j.append([(d.iloc[i]['x'] - all_mean[0]) / all_stdev[0], \
                      (d.iloc[i+1]['x'] - d.iloc[i]['x'] - mean[0]) / std[0],\
                      (d.iloc[i]['y'] - all_mean[1]) / all_stdev[1], \
                      (d.iloc[i+1]['y'] - d.iloc[i]['y'] - mean[1]) / std[1],\
                      (d.iloc[i]['z'] - all_mean[2]) / all_stdev[2], \
                      (d.iloc[i+1]['z'] - d.iloc[i]['z'] - mean[2]) / std[2], \
                      (d.iloc[i]['r'] - all_mean[3]) / all_stdev[3], \
                      (d.iloc[i+1]['r'] - d.iloc[i]['r'] - mean[3]) / std[3]])
              # j.append([(d.iloc[i+1]['x'] - d.iloc[i]['x'] - mean[0]) / std[0], \
              #           (d.iloc[i+1]['y'] - d.iloc[i]['y'] - mean[1]) / std[1], \
              #           (d.iloc[i+1]['z'] - d.iloc[i]['z'] - mean[2]) / std[2], \
              #           (d.iloc[i+1]['r'] - d.iloc[i]['r'] - mean[3]) / std[3]])
              # #########################
            # j.append([(d.iloc[i]['x'] - all_mean[0]) / all_stdev[0], \
            #           (d.iloc[i]['y'] - all_mean[1]) / all_stdev[1], \
            #           (d.iloc[i]['z'] - all_mean[2]) / all_stdev[2], \
            #           (d.iloc[i]['r'] - all_mean[3]) / all_stdev[3]])
            # j.append([(d[i]- all_mean) / all_stdev[0]])
          j = torch.tensor(j).float()
          x_res.append(j)
          label.append(y)
    return x_res, label

def normalize_data(df_train):
  all_mean = []
  all_stdev = []

  for c in df_train.columns:
      mean = df_train[c].mean()
      stdev = df_train[c].std()
      all_mean.append(mean)
      all_stdev.append(stdev)

  return all_mean, all_stdev

def data_preprocess(file_name, out):
  min_max_scaler = preprocessing.StandardScaler()
  print(file_name)
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                  & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                  &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  all_mean, all_stdev = normalize_data(data_n)
  X, Y = unroll_4(data_n, 10, all_mean, all_stdev, out)

  return X, Y

class SequenceDataset(Dataset):
    def __init__(self, x):
        self.X = x

    def __len__(self):
        return len(self.X)

    def __getitem__(self, i): 

        return self.X[i]

set_seed(1)
batch_size = 16
X = []
Y = []
for i in range(1,6):
  '''
  file_name = 'danny/danny flight ' + str(i) + '.csv'
  out = 1
  x, y = data_preprocess(file_name, out)
  X += x
  Y += y
  '''
  # file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  file_name = 'danny/danny flight ' + str(i) + '.csv'
  out = 0
  x, y = data_preprocess(file_name, out)
  X += x
dataset = SequenceDataset(X)
torch.manual_seed(99)
train_loader = torch.utils.data.DataLoader(dataset = dataset,
                                    batch_size = batch_size,
                                    shuffle = True)

for x in train_loader:
  print(x.shape)
  break

#@title Train
from torch import nn

class Encoder(nn.Module):
  def __init__(self, n_features, embedding_dim):
    super(Encoder, self).__init__()
    self.n_features = n_features
    self.num_layers = 2
    self.embedding_dim, self.hidden_dim = embedding_dim, embedding_dim
    self.rnn1 = nn.LSTM(
      input_size=n_features,
      hidden_size=self.hidden_dim,
      num_layers=self.num_layers,
      batch_first=True
    )
    # self.rnn2 = nn.LSTM(
    #   input_size=self.hidden_dim,
    #   hidden_size=embedding_dim,
    #   num_layers=1,
    #   batch_first=True
    # )
    torch.nn.init.xavier_uniform_(self.rnn1.weight_ih_l0)
    torch.nn.init.xavier_normal_(self.rnn1.weight_ih_l1)
  def forward(self, x):
    # x = x.reshape((1, self.seq_len, self.n_features))
    batch_size = x.shape[0]
    h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).requires_grad_()
    c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).requires_grad_()
    
    x, (hidden_n, _) = self.rnn1(x, (h0, c0))
    # x, (hidden_n, _) = self.rnn2(x)
    # return hidden_n.reshape((self.n_features, self.embedding_dim))
    # return hidden_n[1]
    return x
    
class Decoder(nn.Module):
  def __init__(self, n_features, input_dim):
    super(Decoder, self).__init__()
    self.input_dim = input_dim
    self.hidden_dim = 8
    self.num_layers = 2
    self.n_features = n_features
    self.rnn1 = nn.LSTM(
      input_size=input_dim,
      hidden_size=self.hidden_dim,
      num_layers=2,
      batch_first=True
    )
    # self.rnn2 = nn.LSTM(
    #   input_size=input_dim,
    #   hidden_size=self.hidden_dim,
    #   num_layers=1,
    #   batch_first=True
    # )
    torch.nn.init.xavier_uniform_(self.rnn1.weight_ih_l0)
    torch.nn.init.xavier_normal_(self.rnn1.weight_ih_l1)
    self.output_layer = nn.Linear(self.hidden_dim, n_features)
    torch.nn.init.xavier_uniform_(self.output_layer.weight)

  def forward(self, x):
    # x = x.repeat(self.seq_len, self.n_features)
    # x = x.reshape((self.n_features, self.seq_len, self.input_dim))
    batch_size = x.shape[0]
    h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).requires_grad_()
    c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).requires_grad_()
    x, (hidden_n, cell_n) = self.rnn1(x, (h0, c0))
    # x, (hidden_n, cell_n) = self.rnn2(x)
    # x = x.reshape((self.seq_len, self.hidden_dim))
    return self.output_layer(x)

class RecurrentAutoencoder(nn.Module):
  def __init__(self, seq_len, n_features, embedding_dim):
    super(RecurrentAutoencoder, self).__init__()
    self.encoder = Encoder(n_features, embedding_dim)
    self.decoder = Decoder(n_features, embedding_dim)
  def forward(self, x):
    x = self.encoder(x)
    x = self.decoder(x)
    return x

def test_model(data_loader, model, loss_function):
    num_batches = len(data_loader)
    total_loss = 0
    num_dismatch = 0
    outputs = []
    model.eval()
    with torch.no_grad():
        for X in data_loader:
            output = model.forward(X)
            outputs.append(output)
            l = loss_function(output, X)
            total_loss += l.item()

    avg_loss = total_loss / num_batches
    return avg_loss, outputs

def train_model(loss_function):
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  print("The model will be running on", device, "device")
  num_hidden_units = 1
  model = RecurrentAutoencoder(10, 8, 4)
  model = model.to(device)
  optimizer = torch.optim.Adam(model.parameters(),
                                  lr = 1e-2,
                                  weight_decay = 1e-6)
  epochs = 20
  train_loss = []
  val_loss = []
  # Convert model parameters and buffers to CPU or Cuda
  model.train()
  num_batches = len(train_loader)
  for epoch in range(epochs):
    losses = 0
    for x in train_loader:
      x = Variable(x.to(device))

      optimizer.zero_grad()
      output = model.forward(x)
      loss = loss_function(output, x)
      loss.backward()
      optimizer.step()
      losses += loss.item()
    
    avg_loss = losses / num_batches
    train_loss.append(avg_loss)
    # l, _ = test_model(val_loader, model, loss_function)
    # val_loss.append(l)
    print("Epoch: ", epoch, "Loss: ", avg_loss)

  return model, train_loss, val_loss

# loss_function = nn.L1Loss(reduction='sum')
loss_function = torch.nn.L1Loss()
# loss_function = torch.nn.BCELoss() 
# loss_function = torch.nn.CrossEntropyLoss()
#Train
model, train_loss, val_loss = train_model(loss_function)

#@title Autoencoder_4 test
batch_size = 1
d_loss = []
f_loss = []
for i in range(6,11):
  X = []
  Y = []  
  file_name = 'danny/danny flight ' + str(i) + '.csv'
  out = 1
  x, y = data_preprocess(file_name, out)
  X += x
  Y += y
  dataset = SequenceDataset(X)
  torch.manual_seed(99)
  data_loader = torch.utils.data.DataLoader(dataset = dataset,
                                      batch_size = batch_size,
                                      shuffle = False)
  total_loss = 0
  avg_loss, outputs = test_model(data_loader, model, loss_function)
  a = [i.numpy() for i in outputs]
  b = []
  prob_d = []
  prob_f = []
  # print('D: ', b.count('D'))
  # print('F: ', b.count('F'))
  # print('D accuracy: ', b.count('D') / (b.count('D') + b.count('F')))
  # print('D avg probability: ', sum(prob_d) / len(prob_d))
  # print('F avg probability: ', sum(prob_f) / len(prob_f))
  print('D loss: ', avg_loss)
  d_loss.append(avg_loss)

  X = []
  Y = []
  file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  out = 0
  x, y = data_preprocess(file_name, out)
  X += x
  Y += y
  dataset = SequenceDataset(X)
  torch.manual_seed(99)
  data_loader = torch.utils.data.DataLoader(dataset = dataset,
                                      batch_size = batch_size,
                                      shuffle = False)
  avg_loss, outputs = test_model(data_loader, model, loss_function)
  # print('avg loss: ', avg_loss)
  a = [i.numpy() for i in outputs]
  b = []
  prob_d = []
  prob_f = []
  # print('D: ', b.count('D'))
  # print('F: ', b.count('F'))
  # print('F accuracy: ', b.count('F') / (b.count('D') + b.count('F')))
  # print('D avg probability: ', sum(prob_d) / len(prob_d))
  # print('F avg probability: ', sum(prob_f) / len(prob_f))
  print('F loss: ', avg_loss) 
  f_loss.append(avg_loss)
  # output: [Fengchen, Danny]
print('D avg loss: ', sum(d_loss) / len(d_loss))
print('F avg loss: ', sum(f_loss) / len(f_loss))

#@title Old autoencoder
from torch.optim import optimizer
def normalize_data(df_train):
  all_mean = []
  all_stdev = []

  for c in df_train.columns:
      mean = df_train[c].mean()
      stdev = df_train[c].std()
      all_mean.append(mean)
      all_stdev.append(stdev)

  return all_mean, all_stdev

def data_preprocess(file_name, batch_size):
  min_max_scaler = preprocessing.StandardScaler()
  X = []
  print(file_name)
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                  & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                  &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  all_mean, all_stdev = normalize_data(data_n)
  data_seq1, x_res1, x_next1, y_res1, z_res1, r_res1 = unroll(data_n, 20)
  for index, j in enumerate(x_res1):
    j = (j - all_mean[0]) / all_stdev[0]
    j = j.reset_index()['x'].tolist()
    num_unique = np.unique(j[0:10])
    if len(num_unique) > 1:
      # j = (j - all_mean[0]) / all_stdev[0]
      x = torch.tensor(j).float()
      X.append(x)
  
  return X

class AE(torch.nn.Module):
  def __init__(self):
    super().__init__()

    self.encoder = torch.nn.Sequential(
        torch.nn.Linear(20, 5), 
        torch.nn.ReLU(),
        # torch.nn.Linear(5, 2)
    )
    self.decoder = torch.nn.Sequential(
        # torch.nn.Linear(2, 5),
        
        torch.nn.Linear(5, 20),
        torch.nn.ReLU(),
    )

  def forward(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    return decoded
  
def test_model(data_loader, model, loss_function):
    
    num_batches = len(data_loader)
    total_loss = 0
    num_dismatch = 0

    model.eval()
    with torch.no_grad():
        for X in data_loader:
            output = model(X)
            l = loss_function(output, X)
            total_loss += l.item()

    avg_loss = total_loss / num_batches
    return avg_loss

def train_model(loss_function):
  batch_size = 16
  X = []
  for i in range(1,5):
    # file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
    file_name = 'danny/danny flight ' + str(i) + '.csv'
    X += data_preprocess(file_name, batch_size)
  loader = torch.utils.data.DataLoader(dataset = X,
                                      batch_size = 32,
                                      shuffle = True)
  model = AE()
  optimizer = torch.optim.Adam(model.parameters(),
                                  lr = 5e-4,
                                  weight_decay = 1e-8)
  epochs = 50
  outputs = []
  losses = []
  for epoch in range(epochs):
    for x in loader:
      reconstructed = model(x)
      loss = loss_function(reconstructed, x)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      losses.append(loss)
    outputs.append((epochs, x, reconstructed))
    print("Epoch: ", epoch, "Loss: ", loss.item())
  
  return model

loss_function = torch.nn.MSELoss()
#Train
model = train_model(loss_function)

#@title Default title text
# Test
batch_size = 1
for i in range(6,7):
  file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  # file_name = 'danny/danny flight ' + str(i) +'.csv'
  test_loader = data_preprocess(file_name, batch_size)

  avg_loss = test_model(test_loader, model, loss_function)
  print('Test loss: ', avg_loss)



"""Aritificial neural network"""

#@title Default title text
from torch.optim import optimizer
from sklearn.model_selection import train_test_split 
from torch.utils.data import Dataset
from torch.autograd import Variable

def normalize_data(df_train):
  all_mean = []
  all_stdev = []

  for c in df_train.columns:
      mean = df_train[c].mean()
      stdev = df_train[c].std()
      all_mean.append(mean)
      all_stdev.append(stdev)

  return all_mean, all_stdev

def data_preprocess(file_name, batch_size, out):
  min_max_scaler = preprocessing.StandardScaler()
  X = []
  Y = []
  print(file_name)
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                  & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                  &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  all_mean, all_stdev = normalize_data(data_n)
  data_seq1, x_res1, x_next1, y_res1, z_res1, r_res1 = unroll(data_n, 10)
  y = [0, 0]
  y[out] = 1
  for index, j in enumerate(x_res1):
    num_unique = np.unique(j[0:10])
    if len(num_unique) > 1:
      j = j.reset_index()['x'].tolist()
      j = (j - all_mean[0]) / all_stdev[0]
      x = torch.tensor(j).float()
      X.append(x)
      Y.append(torch.tensor(y).float())
  
  return X, Y
class SequenceDataset(Dataset):
    def __init__(self, x, y):
        self.y = y
        self.X = x

    def __len__(self):
        return len(self.X)

    def __getitem__(self, i): 

        return self.X[i], self.y[i]
batch_size = 10
X = []
Y = []
for i in range(1,10):
  file_name = 'danny/danny flight ' + str(i) + '.csv'
  out = 1
  x, y = data_preprocess(file_name, batch_size, out)
  X += x
  Y += y
  file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  out = 0
  x, y = data_preprocess(file_name, batch_size, out)
  X += x
  Y += y
dataset = SequenceDataset(X, Y)
loader = torch.utils.data.DataLoader(dataset = dataset,
                                    batch_size = batch_size,
                                    shuffle = True)

#@title Default title text
# Reading the cleaned numeric titanic survival data
import pandas as pd
import numpy as np

# To remove the scientific notation from numpy arrays
np.set_printoptions(suppress=True)

TitanicSurvivalDataNumeric=pd.read_pickle('TitanicSurvivalDataNumeric.pkl')
TitanicSurvivalDataNumeric.head()
# Separate Target Variable and Predictor Variables
TargetVariable=['Survived']
Predictors=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',
            'Embarked_C', 'Embarked_Q', 'Embarked_S']

X=TitanicSurvivalDataNumeric[Predictors].values
y=TitanicSurvivalDataNumeric[TargetVariable].values


### Sandardization of data ###
### We does not standardize the Target variable for classification
from sklearn.preprocessing import StandardScaler
PredictorScaler=StandardScaler()

# Storing the fit object for later reference
PredictorScalerFit=PredictorScaler.fit(X)

# Generating the standardized values of X and y
X=PredictorScalerFit.transform(X)

# Split the data into training and testing set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Quick sanity check with the shapes of Training and Testing datasets
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
dataset = SequenceDataset(X_train, y_train)
loader = torch.utils.data.DataLoader(dataset = dataset,
                                    batch_size = 5)

#@title Default title text
class Classification(torch.nn.Module):
  def __init__(self):
    def init_weights(m):
      if isinstance(m, torch.nn.Linear):
          torch.nn.init.xavier_uniform_(m.weight)
          m.bias.data.fill_(0.01)
    super(Classification, self).__init__()

    self.model = torch.nn.Sequential(
        # torch.nn.Linear(10, 6), 
        # torch.nn.ReLU(),
        # torch.nn.Linear(6, 3), 
        # torch.nn.ReLU(),
        torch.nn.Linear(10, 2),
        torch.nn.Sigmoid(),
    )
    self.model.apply(init_weights)

  def forward(self, x):
    output = self.model(x)
    return output

  # output: [Fengchen, Danny]
  # Is danny = 1
  # Not danny = 0
def train_model(loss_function):
  model = Classification()
  optimizer = torch.optim.Adam(model.parameters(),
                                  lr = 1e-2,
                                  weight_decay = 1e-4)
  epochs = 80
  outputs = []
  losses = []
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  print("The model will be running on", device, "device")
  # Convert model parameters and buffers to CPU or Cuda
  model.to(device)
  model.train()
  num = len(loader)
  for epoch in range(epochs):
    losses = 0
    for x, y in loader:
      x = Variable(x.to(device))
      y = Variable(y.to(device))

      optimizer.zero_grad()
      output = model(x)
      loss = loss_function(output, y)
      loss.backward()
      optimizer.step()
      losses += loss.item()
    
    outputs.append((epochs, y, output))
    print("Epoch: ", epoch, "Loss: ", losses / num)

  return model

# loss_function = torch.nn.BCELoss() 
loss_function = torch.nn.CrossEntropyLoss()
#Train
model = train_model(loss_function)

#@title Default title text
def test_model(data_loader, model, loss_function):
    
    num_batches = len(data_loader)
    total_loss = 0
    num_dismatch = 0
    outputs = []
    model.eval()
    with torch.no_grad():
        for X,y in data_loader:
            output = model(X)
            outputs.append(output)
            l = loss_function(output, y)
            total_loss += l.item()

    avg_loss = total_loss / num_batches
    return avg_loss, outputs

X= []
Y = []
batch_size = 1
for i in range(1,11):
  X= []
  Y = []
  # print(len(X), len(Y))
  # file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  file_name = 'danny/danny flight ' + str(i) + '.csv'
  out = 1
  x, y = data_preprocess(file_name, batch_size, out)
  X += x
  Y += y
    # file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
    # out = 0
    # x, y = data_preprocess(file_name, batch_size, out)
    # X += x
    # Y += y
  # X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1) # 70% training and 30% test  
  # print(len(X), len(Y))
  dataset = SequenceDataset(X, Y)
  data_loader = torch.utils.data.DataLoader(dataset = dataset,
                                      batch_size = 1,
                                      shuffle = False)
  avg_loss, outputs = test_model(data_loader, model, loss_function)
  # avg_loss
  a = [i.numpy() for i in outputs]
  b = []
  for c in a:
    if c[0][0] < c[0][1]:
      b.append('D')
    else:
      b.append('F')
  print('D: ', b.count('D'))
  print('F: ', b.count('F'))
  # print('F accuracy: ', b.count('F') / (b.count('D') + b.count('F')))
  print('D accuracy: ', b.count('D') / (b.count('D') + b.count('F')))



"""Visual difference"""

# test

fig,ax = plt.subplots()
a = np.arange (0.2, 1, 0.2)
c = ['r', 'g', 'b', 'cyan']
for i in range(5,6):
  # file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  file_name = 'danny/danny flight ' + str(i) + '.csv'
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                    & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                    &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  data_seq, x_res, y_res, z_res = unroll(data_n, 10)
  for j in range(len(x_res)):
    v =x_res[j].reset_index()['x']
    ax.plot(v - v[0],color='r', alpha = 0.1)
plt.show()

#@title Default title text
fig,ax = plt.subplots()
a = np.arange (0.2, 1, 0.2)
c = ['r', 'g', 'b', 'cyan']
for i in range(9,10):
  file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  # file_name = 'danny/danny flight ' + str(i) + '.csv'
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                    & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                    &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  data_seq, x_res, y_res, z_res = unroll(data_n, 10)
  for j in range(len(x_res)):
    v =x_res[j].reset_index()['x']
    ax.plot(v - v[0],color='r', alpha = 0.1)
plt.show()

fig,ax = plt.subplots()
a = np.arange (0.2, 1, 0.2)
c = ['r', 'g', 'b', 'cyan']
for i in range(7,8):
  file_name = 'fengchen/fengchen flight' + str(i) + '.csv'
  # file_name = 'danny/danny flight ' + str(i) + '.csv'
  df = pd.read_csv(file_name)
  data_n = df[['x', 'y', 'z', 'r']]
  data_n = data_n[(data_n['z'] < 1001) & (data_n['z'] > -1001) & (data_n['x'] < 1001) \
                    & (data_n['x'] > -1001) & (data_n['y'] < 1001) & (data_n['y'] > -1001) \
                    &  (data_n['r'] < 1001) & (data_n['r'] > -1001)]
  data_seq, x_res, y_res, z_res = unroll(data_n, 10)
  for j in range(len(x_res)):
    v =x_res[j].reset_index()['x']
    ax.plot(v - v[0],color='r', alpha = 0.1)
plt.show()











"""Keras LSTM"""

# specific libraries for RNN
# keras is a high layer build on Tensorflow layer to stay in high level/easy implementation
from keras.layers.core import Dense, Activation, Dropout
from keras.layers import LSTM
from keras.models import Sequential
import time #helper libraries
from keras.models import model_from_json
import sys
!pip3 install tslearn

# Build the model
model = Sequential()

model.add(LSTM(
    # input_dim=4,
    input_shape = (5,4),
    # output_dim=50,
    # units = 128,
    units = 5,
    # units = 4,
    # return_sequences=True))
    return_sequences=False))
# model.add(Dropout(0.2))

# model.add(LSTM(
#     # 100,
#     20,
#     return_sequences=False))
# model.add(Dropout(0.2))

model.add(Dense(
    units=4))
# model.add(Activation('linear'))

start = time.time()
model.compile(loss='mse', optimizer='adam')
print('compilation time : {}'.format(time.time() - start))
model.summary()

#unroll: create sequence of 50 previous data points for each data points
def unroll(data,sequence_length=24):
    result = []
    for index in range(len(data) - sequence_length):
        result.append(data[index: index + sequence_length])
    return np.asarray(result)

min_max_scaler = preprocessing.StandardScaler()
# important parameters and train/test size
prediction_time = 1 
testdatasize = 0
unroll_length = 5
testdatacut = testdatasize + unroll_length  + 1

# for i in range(1,6):
  # file_name = 'danny/danny flight ' + str(i) + '.csv'
file_name = 'fengchen/fengchen flight1' + '.csv'
print(file_name)
df = pd.read_csv(file_name)
data_n = df[['x', 'y', 'z', 'r']]
# data_n = data_n[data_n['x'] != 0]
data_n = data_n[(data_n['z'] < 1000) & (data_n['z'] > -1000) & (data_n['x'] < 1000) \
                  & (data_n['x'] > -1000) & (data_n['y'] < 1000) & (data_n['y'] > -1000) \
                  &  (data_n['r'] < 1000) & (data_n['r'] > -1000)]
np_scaled = min_max_scaler.fit_transform(data_n)
data_n = pd.DataFrame(np_scaled)

#train data
x_train = data_n[0:-prediction_time]
y_train = data_n[testdatacut:len(data_n)]
# adapt the datasets for the sequence data shape
x_train = unroll(x_train,unroll_length)
y_train = y_train[-x_train.shape[0]:]

# see the shape
print("x_train", x_train.shape)
print("y_train", y_train.shape)
# Train the model
#nb_epoch = 350

model.fit(
    x_train,
    y_train,
    # batch_size=len(x_train),
    batch_size = 16,
    epochs=80,
    # validation_split=0.1
    )

# file_name = 'fengchen/fengchen flight6' + '.csv'
file_name = 'danny/danny flight 6.csv'
print(file_name)
df = pd.read_csv(file_name)
data_n = df[['x', 'y', 'z', 'r']]
# data_n = data_n[data_n['x'] != 0]
data_n = data_n[(data_n['z'] < 1000) & (data_n['z'] > -1000) & (data_n['x'] < 1000) \
                  & (data_n['x'] > -1000) & (data_n['y'] < 1000) & (data_n['y'] > -1000) \
                  &  (data_n['r'] < 1000) & (data_n['r'] > -1000)]
np_scaled = min_max_scaler.fit_transform(data_n)
data_n = pd.DataFrame(np_scaled)

#train data
x_test = data_n[0:-prediction_time]
y_test = data_n[testdatacut:len(data_n)]
# adapt the datasets for the sequence data shape
x_test = unroll(x_test,unroll_length)
y_test = y_test[-x_test.shape[0]:]
p = model.predict(x_test)
from sklearn.metrics import mean_squared_error
  
# Calculation of Mean Squared Error (MSE)
mean_squared_error(y_test,p)

file_name2 = 'fengchen/fengchen flight1.csv'
df2 = pd.read_csv(file_name2)
data_n2 = df2[['x', 'y', 'z', 'r']]
print(data_n2[(data_n2['x'] != 0) & (data_n2['z'] < 1000)]['x'])

import random
i = random.choice([0,1])
j = random.choice([6,7,8,9,10])
if i == 0:
    file_name = 'fengchen/fengchen flight' + str(j) + '.csv'
else:
    file_name = 'danny/danny flight ' + str(j) + '.csv'
df = pd.read_csv(file_name)
data_n2 = df[['x', 'y', 'z', 'r']]
# fig, (ax1, ax2) = plt.subplots(2,1)
fig, ax = plt.subplots()
ax.plot(data_n2[(data_n2['x'] != 0) & (data_n2['z'] < 1000) & (data_n2['z'] > -1000) & (data_n2['x'] < 1000) \
                 & (data_n2['x'] > -1000) & (data_n2['y'] < 1000) & (data_n2['y'] > -1000) &  (data_n2['r'] < 1000) & (data_n2['r'] > -1000)])
ax.legend(['x', 'y', 'z', 'r'])
plt.show()

file_name = 'danny/danny flight 5.csv'
df = pd.read_csv(file_name)
data_n = df[['x', 'y', 'z', 'r']]
fig, (ax1, ax2) = plt.subplots(2,1)
# fig, ax = plt.subplots()

# a = df.loc[df['anomaly26'] == 1, ['time_epoch', 'value']] #anomaly

# ax.plot(df['time_epoch'], df['value'], color='blue')
# ax.scatter(a['time_epoch'],a['value'], color='red')
ax1.plot(data_n[(data_n['x'] != 0) & (data_n['z'] < 1000) & (data_n['z'] > -1000) & (data_n['x'] < 1000) \
                 & (data_n['x'] > -1000) & (data_n['y'] < 1000) & (data_n['y'] > -1000) &  (data_n['r'] < 1000) & (data_n['r'] > -1000)])
ax1.legend(['x', 'y', 'z', 'r'])
# ax.legend(['First line', 'Second line'])
# plt.show()
file_name2 = 'fengchen/fengchen flight5.csv'
# file_name2 = 'danny/danny flight 5.csv'

df2 = pd.read_csv(file_name2)
data_n2 = df2[['x', 'y', 'z', 'r']]
ax2.plot(data_n2[(data_n2['x'] != 0) & (data_n2['z'] < 1000) & (data_n2['z'] > -1000) & (data_n2['x'] < 1000) \
                 & (data_n2['x'] > -1000) & (data_n2['y'] < 1000) & (data_n2['y'] > -1000) &  (data_n2['r'] < 1000) & (data_n2['r'] > -1000)])
ax2.legend(['x', 'y', 'z', 'r'])
plt.show()

file_name

len(x_train)

# # Train the model
# #nb_epoch = 350

# model.fit(
#     x_train,
#     y_train,
#     batch_size=3028,
#     epochs=30,
#     # validation_split=0.1
#     )

# df_test = pd.read_csv("danny/danny flight 6.csv")
df_test = pd.read_csv("fengchen/fengchen flight9.csv")
data_n_test = df_test[['x', 'y', 'z', 'r']]
data_n_test = data_n_test[data_n_test['x'] != 0]
min_max_scaler = preprocessing.StandardScaler()
np_scaled_test = min_max_scaler.fit_transform(data_n_test)
data_n_test = pd.DataFrame(np_scaled_test)

# important parameters and train/test size
prediction_time = 1 
testdatasize = len(data_n_test)
unroll_length = 10
testdatacut = testdatasize + unroll_length  + 1
#train data
x_test = data_n_test[0-testdatacut:-prediction_time]
y_test_n = data_n_test[prediction_time-testdatacut:  ]
x_test  = unroll(x_test,unroll_length)
y_test  = y_test_n[-x_test.shape[0]:]

df_test = pd.read_csv("danny/danny flight 4.csv")
data_n_test = df_test[['x', 'y', 'z', 'r']]
data_n_test = data_n_test[data_n_test['x'] != 0]
min_max_scaler = preprocessing.StandardScaler()
np_scaled_test = min_max_scaler.fit_transform(data_n_test)
data_n_test = pd.DataFrame(np_scaled_test)

# important parameters and train/test size
prediction_time = 1 
testdatasize = len(data_n_test)
unroll_length = 10
testdatacut = testdatasize + unroll_length  + 1
#train data
x_test = data_n_test[0-testdatacut:-prediction_time]
y_test_n = data_n_test[prediction_time-testdatacut:  ]
x_test  = unroll(x_test,unroll_length)
y_test  = y_test_n[-x_test.shape[0]:]

print("x_test", x_test.shape)
print("y_test", y_test.shape)

json_file = open('model2.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("model2.h5")
print("Loaded model from disk")

# create the list of difference between prediction and test data
loaded_model = model
diff=[]
ratio=[]
p = loaded_model.predict(x_test)
# predictions = lstm.predict_sequences_multiple(loaded_model, x_test, 50, 50)
for u in range(len(y_test)):
    pr = p[u]
    # ratio.append((y_test[u+len(data_n_test) - x_test.shape[0]]/pr)-1)
    # ratio.append((y_test.iloc[u] / p) - 1)
    ratio.append((pr / y_test.iloc[u]) - 1)  #what ratio if the diff
    # diff.append(abs(y_test[u+len(data_n_test) - x_test.shape[0]]- pr))
    # diff.append((y_test.iloc[u] - pr))  # real diff
    diff.append(np.linalg.norm((y_test.iloc[u] - pr), ord=1))

##Danny test
x = [i[0] for i in ratio]
y = [i[1] for i in ratio]
z = [i[2] for i in ratio]
r = [i[3] for i in ratio]
print("Danny")
print(sum(x) / len(x), sum(y) / len(y), sum(z) / len(z), sum(r) / len(r))
print(sum(diff) / len(diff))

##Fengchen test
x = [i[0] for i in ratio]
y = [i[1] for i in ratio]
z = [i[2] for i in ratio]
r = [i[3] for i in ratio]
print("Fengchen")
print(sum(x) / len(x), sum(y) / len(y), sum(z) / len(z), sum(r) / len(r))
print(sum(diff) / len(diff))

from sklearn.metrics import mean_squared_error
  
# Calculation of Mean Squared Error (MSE)
mean_squared_error(y_test,p)

np.average(ratio)

# plot the prediction and the reality (for the test data)
fig, axs = plt.subplots()
axs.plot(p,color='red', label='prediction')
axs.plot(y_test,color='blue', label='y_test')
plt.legend(loc='upper left')
plt.show()

#####Fengchen
# plot the prediction and the reality (for the test data)
fig, axs = plt.subplots()
axs.plot(p,color='red', label='prediction')
axs.plot(y_test,color='blue', label='y_test')
plt.legend(loc='upper left')
plt.show()

outliers_fraction = 0.01
diff = pd.Series(diff)
number_of_outliers = int(outliers_fraction*len(diff))
threshold = diff.nlargest(number_of_outliers).min()
print('len of diff: ', len(diff), 'threshold: ', threshold)
# data with anomaly label (test data part)
test = (diff >= threshold).astype(int)
# the training data part where we didn't predict anything (overfitting possible): no anomaly
# complement = pd.Series(0, index=np.arange(len(x_train)))
complement = pd.Series()
# # add the data to the main
df['anomaly27'] = complement.append(test, ignore_index='True')
print(df['anomaly27'].value_counts())

# serialize model to JSON
model_json = model.to_json()
with open("model2.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model2.h5")
print("Saved model to disk")

def unroll(data,sequence_length=24):
    result = []
    for index in range(len(data) - sequence_length):
        result.append(data[index: index + sequence_length])
    return np.asarray(result)
def flatten(l):
    return np.array([item for sublist in l for item in sublist])

def plot4d (data, color):
  markercolor = data[:,3]
  return go.Scatter3d(x=data[:,0],
                    y=data[:,1],
                    z=data[:,2],
                    marker=dict(color=markercolor,
                                opacity=1,
                                reversescale=True,
                                colorscale=color,
                                size=5),
                    line=dict (width=0.02),
                    mode='markers')
from tslearn.utils import to_time_series
from sklearn.preprocessing import StandardScaler
import pandas as pd
import plotly
import plotly.graph_objs as go
from tslearn.clustering import TimeSeriesKMeans
scaler = StandardScaler()
seed = 1
file_name = 'danny/danny flight 1.csv'
# file_name = 'fengchen/fengchen flight4.csv'
df = pd.read_csv(file_name)
data_n = df[['x', 'y', 'z', 'r']]
data_n = data_n[(data_n['z'] < 1000) & (data_n['z'] > -1000) & (data_n['x'] < 1000) \
                 & (data_n['x'] > -1000) & (data_n['y'] < 1000) & (data_n['y'] > -1000) &  (data_n['r'] < 1000) & (data_n['r'] > -1000)]
# file_name2 = 'danny/danny flight 5.csv'
file_name2 = 'fengchen/fengchen flight1.csv'
df2 = pd.read_csv(file_name2)
data_n2 = df2[['x', 'y', 'z', 'r']]
data_n2 = data_n2[(data_n2['x'] != 0) & (data_n2['z'] < 1000) & (data_n2['z'] > -1000) & (data_n2['x'] < 1000) \
                 & (data_n2['x'] > -1000) & (data_n2['y'] < 1000) & (data_n2['y'] > -1000) &  (data_n2['r'] < 1000) & (data_n2['r'] > -1000)]
data_n2 = scaler.fit_transform(data_n2)
data_n = scaler.fit_transform(data_n)
data_n_formatted = to_time_series(data_n)
data_n_formatted2 = to_time_series(data_n2)
# model = TimeSeriesKMeans(n_clusters=3, metric="dtw", max_iter=10)
# model.fit(data_n_formatted)
dba_km = TimeSeriesKMeans(n_clusters=3,
                          n_init=2,
                          metric="dtw",
                          verbose=True,
                          max_iter_barycenter=10,
                          random_state=seed)
y_pred = dba_km.fit_predict(data_n_formatted)
y_pred2 = dba_km.fit_predict(data_n_formatted2)

#Set marker properties
# for i in u_labels:
#     plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)
# plt.legend()
# plt.show()
fig1 = plot4d(data_n[y_pred == 0], 'Reds')
fig2 = plot4d(data_n[y_pred == 1], 'Blues')
fig3 = plot4d(data_n[y_pred == 2], 'purples')
fig4 = plot4d(data_n2[y_pred2 == 0], 'magenta')
fig5 = plot4d(data_n2[y_pred2 == 1], 'blugrn')
fig6 = plot4d(data_n2[y_pred2 == 2], 'purp')

#Make Plot.ly Layout
mylayout = go.Layout(scene=dict(xaxis=dict( title="x"),
                                yaxis=dict( title="y"),
                                zaxis=dict(title="z")),)

fig = go.Figure(
    data=[fig1, fig2, fig3,fig4,fig5,fig6],
    # layout_title_text=mylayout
)
fig.show()

seed = 0
file_name = 'danny/danny flight 4.csv'
# file_name = 'fengchen/fengchen flight5.csv'
df = pd.read_csv(file_name)
data_n = df[['x', 'y', 'z', 'r']]
data_n = data_n[(data_n['z'] < 1000) & (data_n['z'] > -1000) & (data_n['x'] < 1000) \
                 & (data_n['x'] > -1000) & (data_n['y'] < 1000) & (data_n['y'] > -1000) &  (data_n['r'] < 1000) & (data_n['r'] > -1000)]
data_n = scaler.fit_transform(data_n)
sz = data_n.shape[1]
# Euclidean k-means
print("Euclidean k-means")
km = TimeSeriesKMeans(n_clusters=3, verbose=True, random_state=seed)
y_pred = km.fit_predict(data_n)

plt.figure()
for yi in range(3):
    plt.subplot(3, 3, yi + 1)
    for xx in data_n[y_pred == yi]:
        plt.plot(xx.ravel(), "k-", alpha=.2)
    plt.plot(km.cluster_centers_[yi].ravel(), "r-")
    plt.xlim(0, sz)
    plt.ylim(-4, 4)
    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),
             transform=plt.gca().transAxes)
    if yi == 1:
        plt.title("Euclidean $k$-means")

# DBA-k-means
print("DBA k-means")
dba_km = TimeSeriesKMeans(n_clusters=3,
                          n_init=2,
                          metric="dtw",
                          verbose=True,
                          max_iter_barycenter=10,
                          random_state=seed)
y_pred = dba_km.fit_predict(data_n)

for yi in range(3):
    plt.subplot(3, 3, 4 + yi)
    for xx in data_n[y_pred == yi]:
        plt.plot(xx.ravel(), "k-", alpha=.2)
    plt.plot(dba_km.cluster_centers_[yi].ravel(), "r-")
    plt.xlim(0, sz)
    plt.ylim(-4, 4)
    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),
             transform=plt.gca().transAxes)
    if yi == 1:
        plt.title("DBA $k$-means")

# Soft-DTW-k-means
print("Soft-DTW k-means")
sdtw_km = TimeSeriesKMeans(n_clusters=3,
                           metric="softdtw",
                           metric_params={"gamma": .01},
                           verbose=True,
                           random_state=seed)
y_pred = sdtw_km.fit_predict(data_n)

for yi in range(3):
    plt.subplot(3, 3, 7 + yi)
    for xx in data_n[y_pred == yi]:
        plt.plot(xx.ravel(), "k-", alpha=.2)
    plt.plot(sdtw_km.cluster_centers_[yi].ravel(), "r-")
    plt.xlim(0, sz)
    plt.ylim(-4, 4)
    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),
             transform=plt.gca().transAxes)
    if yi == 1:
        plt.title("Soft-DTW $k$-means")

plt.tight_layout()
plt.show()

res = unroll(data_n, 5)
flatten(res)[:,0]

# danny 5
print(model.cluster_centers_)
print(model.inertia_)